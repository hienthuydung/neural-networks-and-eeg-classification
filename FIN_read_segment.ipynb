{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import pyedflib\n",
    "import datetime\n",
    "import numpy as np\n",
    "import codecs\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import zscore\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import os\n",
    "import fnmatch\n",
    "%matplotlib notebook\n",
    "\n",
    "Edf_file = \"C:\\\\Users\\\\hien\\\\Desktop\\\\EEG_annotations\\\\0-4.edf\"\n",
    "# source = '1519'\n",
    "with pyedflib.EdfReader(Edf_file) as f:\n",
    "        Edf_start = f.getStartdatetime() # returns datetime.datetime object\n",
    "        print(\"Got Startdatetime for\", Edf_file)\n",
    "        Num_annot = f.annotations_in_file # returns integer\n",
    "        print(\"Got number of annotations in\", Edf_file)\n",
    "        print(\"***\", Num_annot, \"annotations in\", Edf_file,\"***\")\n",
    "        Annots = f.readAnnotations() # returns array \n",
    "        print(\"Read in annotations in\", Edf_file)\n",
    "\n",
    "\n",
    "        print(\"Processing\", Edf_file, \"with\", Num_annot, \"annotations...\")\n",
    "\n",
    "\n",
    "        print(\"Entering Num_annot loop\")\n",
    "        for i in range(Num_annot):\n",
    "            \n",
    "            print(\"Entered Num_annot loop\")\n",
    "            # Get datetime.datetime object with absolute event onset \n",
    "            Event_start = Edf_start + datetime.timedelta(seconds=Annots[0][i])\n",
    "            print(\"Got Event_start\", Event_start)\n",
    "\n",
    "            #Edf_start = f.getStartdatetime()\n",
    "\n",
    "            Start = float(Annots[0][i])\n",
    "            Duration = float(f.readAnnotations()[1][i])\n",
    "            End = Start + Duration\n",
    "            Fs = f.getSampleFrequency(0)\n",
    "            print(\"Start:\", Start, \"Type\", type(Start))\n",
    "            print(\"Duration:\", Duration, \"Type\", type(Duration))\n",
    "            print(\"End: \", End, \"Type\", type(End))\n",
    "            print(\"Fs:\", Fs)\n",
    "            print(np.str(Annots[2][i]))\n",
    "\n",
    "            print(Annots[2][i])\n",
    "                \n",
    "            print(\"Gathering event signal data\")\n",
    "            data = f.readSignal(0)[int( (Start) * Fs) : int( (End) * Fs)]\n",
    "            print(\"data has\", len(data), \"data points\")\n",
    "            np.savetxt(\"C:\\\\Users\\\\hien\\\\\\Desktop\\\\EEG_annotations\"\\\n",
    "                       + \"sep09_anno_{}_\".format(i) + np.str(Annots[2][i])+\".csv\".format(i), data, \\\n",
    "                        header=np.str(Annots[2][i]) +np.str(Start)+'-dur-'+np.str(Duration), fmt=\"%10.5f\")\n",
    "#             , delimiter=','\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def read_data(filename, header=0, ifnorm=True, mono=True, row_data=True):\n",
    "    '''read data from .csv\n",
    "    return:\n",
    "        data: 2d array [seq_len, channel]'''\n",
    "    \n",
    "    data = pd.read_csv(filename, header=header, nrows=None)\n",
    "    data = data.values   ### get data without row_index\n",
    "    if ifnorm:   ### 2 * 10240  normalize the data into [0.0, 1.0]]\n",
    "        data_norm = zscore(data)\n",
    "        data = data_norm\n",
    "    if row_data:\n",
    "        data = data.T\n",
    "    data = np.squeeze(data)   ## shape from [1, seq_len] --> [seq_len,]\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def split_filter_data_with_long_loss(data, save_name='filename', accept_loss_threshold=10, accept_data_len=2048):\n",
    "    '''#### start new filter.\n",
    "    ### 1. discard repeated data if the number of repeatation is below a threshold(10).\n",
    "        ### it shouldn't make a hug difference since the sampling reate is 512Hz\n",
    "    ### 2. then segement the recording with long data loss (if the data loss over a threshold, split the data into segments)\n",
    "    ### 3. discard short recordings\n",
    "    ### 4. leftover segments with no data repeatation and long enough\n",
    "    Param:\n",
    "        data: 1D array with data losses, shape(seq_len,)\n",
    "        accept_loss_threshold: int, below the threshold, the data can be interpolated\n",
    "        acccep_data_len: if after segmentation, the data is shorter than this, will be discarded\n",
    "    return:\n",
    "        data_segs: shape=[num_seg, variable(seq_len), 1]'''\n",
    "\n",
    "    error = data[0:-1] - data[1:]\n",
    "    non_zero_error_ind = np.where(error!=0)[0]   ### those indices where there is no data loss\n",
    "\n",
    "    loss_intervals = non_zero_error_ind[1:] - non_zero_error_ind[0:-1]\n",
    "\n",
    "    # ## if smaller than threshold, it means the data loss is short and make sense to squize out the loss points\n",
    "    long_loss_interval_start = np.where(loss_intervals>accept_loss_threshold)[0]   ##get where there are long(>threshold) data loss\n",
    "    \n",
    "    ### either the recording is seperated by long data loss or not\n",
    "    if long_loss_interval_start.size == 0 and data.size>= accept_data_len: ## the whole sequence is valid\n",
    "        print(\"whole sequence is good\")\n",
    "        data_seg_interp = linear_interpolation(data[0:data.size-data.size%2048])\n",
    "        data_segs = data_seg_interp\n",
    "        for ii in range(data_seg_interp.size//2048):\n",
    "            print(data_seg_interp.size//2048, \"segments\")\n",
    "            seg = data_seg_interp[ii*2048 : (ii+1)*2048]\n",
    "            np.savetxt(os.path.dirname(filename) + \"/segNorm_{}_No{}.csv\"\\\n",
    "                       .format(os.path.basename(filename[0:-4]), ii), \\\n",
    "                       seg, delimiter=',', fmt=\"%10.5f\", comments='')\n",
    "            print(os.path.dirname(filename) + \"/segNorm_{}_No{}.csv\"\\\n",
    "                  .format(os.path.basename(filename[0:-4]), ii))\n",
    "    else:\n",
    "        print(\"segmenting data\")\n",
    "        accepted_segs_ind = np.array(np.split( non_zero_error_ind, long_loss_interval_start+1))\n",
    "        \n",
    "        data_segs = []\n",
    "        for ii, ind_seg in enumerate(accepted_segs_ind):\n",
    "            if ind_seg.size > accept_data_len:\n",
    "                ### interpolate the missing data\n",
    "                data_seg_interp = linear_interpolation(data[ind_seg])\n",
    "                data_segs.append(data_seg_interp[0:data_seg_interp.size-data_seg_interp.size%accept_data_len])\n",
    "                for ii in range(data_seg_interp.size//2048):\n",
    "                    seg = data_seg_interp[ii*2048 : (ii+1)*2048]\n",
    "                    np.savetxt(os.path.dirname(filename) + \"/segNorm_{}_No{}.csv\".\\\n",
    "                               format(os.path.basename(filename[0:-4]), ii), \\\n",
    "                               seg, delimiter=',', fmt=\"%10.5f\", comments='')\n",
    "            \n",
    "                  ## discard the data at the end\n",
    "# #                 np.savetxt(save_name, np.array(data_segs), delimiter=',', fmt=\"%10.5f\", comments='')\n",
    "                print(\"file saved\")\n",
    "    return data_segs\n",
    "\n",
    "def linear_interpolation(data):\n",
    "    \"\"\"Helper to handle indices and logical indices of repeated data points(data loss points).\n",
    "\n",
    "    Input:\n",
    "        - data, 1d numpy array with possible data loss which appears with repeating previous values\n",
    "    Output:\n",
    "        - nans, logical indices of NaNs\n",
    "        - index, a function, with signature indices= index(logical_indices),\n",
    "          to convert logical indices of NaNs to 'equivalent' indices\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    err = np.insert((data[1:] - data[0:-1]), 0, data[0])   ## the err of the first element is itself\n",
    "    zeros_ind = err == 0\n",
    "    x = lambda z:z.nonzero()[0]\n",
    "\n",
    "    data[zeros_ind] = np.interp(x(zeros_ind) ,x(~zeros_ind), data[~zeros_ind])\n",
    "\n",
    "    return data\n",
    "\n",
    "def find_files(directory, pattern='*.csv', withlabel=False):\n",
    "    '''fine all the files in one directory and assign '1'/'0' to F or N files'''\n",
    "    files = []\n",
    "    for root, dirnames, filenames in os.walk(directory):\n",
    "        for filename in fnmatch.filter(filenames, pattern):\n",
    "            if withlabel:\n",
    "                if 'Data_F' in filename:\n",
    "                    label = '1'\n",
    "                elif 'Data_N' in filename:\n",
    "                    label = '0'\n",
    "                files.append((os.path.join(root, filename), label))\n",
    "            else:  # only get names\n",
    "                files.append(os.path.join(root, filename))\n",
    "#     random.shuffle(files)   # randomly shuffle the files\n",
    "    return files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
